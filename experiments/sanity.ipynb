{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Dict\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "# Abstract base class for agents.\n",
    "class Agent(ABC):\n",
    "    @abstractmethod\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the agent with necessary parameters, state variables,\n",
    "        and internal data structures (e.g., Q-tables or policy networks).\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def reset(self, randomize: bool = True):\n",
    "        \"\"\"\n",
    "        Reset the agent's internal state for a new episode.\n",
    "        Args:\n",
    "            randomize (bool): If True, randomize the initial state.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_state(self):\n",
    "        \"\"\"\n",
    "        Return the current state of the agent.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def choose_action(self, state):\n",
    "        \"\"\"\n",
    "        Choose an action based on the current state using an exploration/exploitation policy.\n",
    "        Args:\n",
    "            state: The current state.\n",
    "        Returns:\n",
    "            The chosen action.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Execute the given action, updating the agent's state.\n",
    "        Args:\n",
    "            action: The action to perform.\n",
    "        Returns:\n",
    "            The new state after the action.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def update(self, state, action, reward, next_state):\n",
    "        \"\"\"\n",
    "        Update the agent's internal parameters based on the observed transition.\n",
    "        Args:\n",
    "            state: The state before taking the action.\n",
    "            action: The action taken.\n",
    "            reward: The reward received.\n",
    "            next_state: The resulting state.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "class Environment(ABC):\n",
    "    @abstractmethod\n",
    "    def reset(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Reset any internal state of the environment and return environment info if needed.\n",
    "        For example, it might return grid parameters.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def compute_rewards(self, agent_states: List[Tuple[int, int]]) -> List[float]:\n",
    "        \"\"\"\n",
    "        Given the list of agent states, compute and return a reward for each agent.\n",
    "        \n",
    "        Args:\n",
    "            agent_states (List[Tuple[int, int]]): List of (x, y) positions for all agents.\n",
    "        \n",
    "        Returns:\n",
    "            List[float]: A list of rewards, one per agent.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def is_terminal(self, agent_states: List[Tuple[int, int]]) -> bool:\n",
    "        \"\"\"\n",
    "        Determine if the environment is in a terminal state based on the agent states.\n",
    "        \n",
    "        Args:\n",
    "            agent_states (List[Tuple[int, int]]): List of agent positions.\n",
    "        \n",
    "        Returns:\n",
    "            bool: True if the simulation should terminate.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "class SingleAgent(Agent):\n",
    "    def __init__(self, grid_size, alpha=0.1, gamma=0.9, epsilon=0.1, seed=0):\n",
    "        \"\"\"\n",
    "        Initialize the single-agent. Asumes a grid world environment.\n",
    "        Args:\n",
    "            grid_size (int): Size of the grid (assumes a square grid).\n",
    "            alpha (float): Learning rate.\n",
    "            gamma (float): Discount factor.\n",
    "            epsilon (float): Exploration rate.\n",
    "            seed (int): Random seed for reproducibility.\n",
    "        \"\"\"\n",
    "        self.grid_size = grid_size\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.num_actions = 5  # Actions: 0:stay, 1:up, 2:down, 3:left, 4:right\n",
    "\n",
    "        # Create a Q-table with shape: (grid_size, grid_size, num_actions)\n",
    "        self.q_table = jnp.zeros((grid_size, grid_size, self.num_actions))\n",
    "        \n",
    "        # Initialize PRNG key for JAX-based randomness.\n",
    "        self.key = jax.random.PRNGKey(seed)\n",
    "        \n",
    "        # Agent's initial position (starting at the center).\n",
    "        self.x = grid_size // 2\n",
    "        self.y = grid_size // 2\n",
    "\n",
    "    def reset(self, randomize: bool = True):\n",
    "        \"\"\"\n",
    "        Reset the agent's position.\n",
    "        Args:\n",
    "            randomize (bool): If True, set a random starting position; otherwise, use the center.\n",
    "        \"\"\"\n",
    "        if randomize:\n",
    "            self.key, subkey = jax.random.split(self.key)\n",
    "            self.x = int(jax.random.randint(subkey, (), 0, self.grid_size))\n",
    "            self.key, subkey = jax.random.split(self.key)\n",
    "            self.y = int(jax.random.randint(subkey, (), 0, self.grid_size))\n",
    "        else:\n",
    "            self.x = self.y = self.grid_size // 2\n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\"\n",
    "        Return the agent's current state as a tuple (x, y).\n",
    "        \"\"\"\n",
    "        return (self.x, self.y)\n",
    "\n",
    "    @jax.jit\n",
    "    def _move(self, pos, action, grid_size):\n",
    "        \"\"\"\n",
    "        Compute the new position based on the current position and action.\n",
    "        Args:\n",
    "            pos (tuple): Current (x, y) position.\n",
    "            action (int): Action index.\n",
    "            grid_size (int): Size of the grid.\n",
    "        Returns:\n",
    "            A tuple representing the new position (x, y).\n",
    "        \"\"\"\n",
    "        x, y = pos\n",
    "        \n",
    "        def stay():\n",
    "            return (x, y)\n",
    "        def up():\n",
    "            return (x, jnp.maximum(y - 1, 0))\n",
    "        def down():\n",
    "            return (x, jnp.minimum(y + 1, grid_size - 1))\n",
    "        def left():\n",
    "            return (jnp.maximum(x - 1, 0), y)\n",
    "        def right():\n",
    "            return (jnp.minimum(x + 1, grid_size - 1), y)\n",
    "        \n",
    "        # jax.lax.switch dispatches based on the action index.\n",
    "        new_pos = jax.lax.switch(action, [stay, up, down, left, right])\n",
    "        return new_pos\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        \"\"\"\n",
    "        Choose an action using an epsilon-greedy policy.\n",
    "        Args:\n",
    "            state (tuple): The current state (x, y).\n",
    "        Returns:\n",
    "            The selected action as an integer.\n",
    "        \"\"\"\n",
    "        self.key, subkey = jax.random.split(self.key)\n",
    "        rand_val = jax.random.uniform(subkey)\n",
    "        if rand_val < self.epsilon:\n",
    "            # Explore: choose a random action.\n",
    "            self.key, subkey = jax.random.split(self.key)\n",
    "            action = int(jax.random.randint(subkey, (), 0, self.num_actions))\n",
    "            return action\n",
    "        else:\n",
    "            # Exploit: choose the action with the highest Q-value.\n",
    "            x, y = state\n",
    "            q_vals = self.q_table[x, y, :]\n",
    "            action = int(jnp.argmax(q_vals))\n",
    "            return action\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Execute an action and update the agent's position.\n",
    "        Args:\n",
    "            action (int): The action to perform.\n",
    "        Returns:\n",
    "            The new state (x, y) after the move.\n",
    "        \"\"\"\n",
    "        current_state = self.get_state()\n",
    "        new_state = self._move(current_state, action, self.grid_size)\n",
    "        # Update the internal position.\n",
    "        self.x, self.y = int(new_state[0]), int(new_state[1])\n",
    "        return new_state\n",
    "\n",
    "    def update(self, state, action, reward, next_state):\n",
    "        \"\"\"\n",
    "        Update the Q-table using the Q-learning update rule.\n",
    "        Args:\n",
    "            state (tuple): The state before the action.\n",
    "            action (int): The action taken.\n",
    "            reward (float): The reward received.\n",
    "            next_state (tuple): The state after the action.\n",
    "        \"\"\"\n",
    "        x, y = state\n",
    "        nx, ny = next_state\n",
    "        \n",
    "        # Current Q-value.\n",
    "        current_q = self.q_table[x, y, action]\n",
    "        # Maximum Q-value in the next state.\n",
    "        max_next_q = jnp.max(self.q_table[nx, ny, :])\n",
    "        \n",
    "        # Q-learning update.\n",
    "        new_q = (1 - self.alpha) * current_q + self.alpha * (reward + self.gamma * max_next_q)\n",
    "        \n",
    "        # Update the Q-table immutably.\n",
    "        self.q_table = self.q_table.at[x, y, action].set(new_q)\n",
    "\n",
    "class GridCenterReward(Environment):\n",
    "    def __init__(self, grid_size: int, center_reward: float = 1.0):\n",
    "        \"\"\"\n",
    "        Initialize the grid environment.\n",
    "        \n",
    "        Args:\n",
    "            grid_size (int): The width/height of the grid.\n",
    "            center_reward (float): Reward given when an agent reaches the center.\n",
    "        \"\"\"\n",
    "        self.grid_size = grid_size\n",
    "        # Represent the center as a JAX array.\n",
    "        self.center = jnp.array([grid_size // 2, grid_size // 2])\n",
    "        self.center_reward = center_reward\n",
    "\n",
    "    def reset(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Reset the environment. This environment is stateless,\n",
    "        so reset simply returns grid info.\n",
    "        \"\"\"\n",
    "        return {\"grid_size\": self.grid_size, \"center\": self.center}\n",
    "\n",
    "    def compute_rewards(self, agent_states: List[Tuple[int, int]]) -> List[float]:\n",
    "        \"\"\"\n",
    "        Award the center reward to any agent that reaches the center.\n",
    "        \n",
    "        Args:\n",
    "            agent_states (List[Tuple[int, int]]): List of (x, y) positions for each agent.\n",
    "            \n",
    "        Returns:\n",
    "            List[float]: A reward for each agent.\n",
    "        \"\"\"\n",
    "        # Convert list of tuples to a JAX array of shape (n_agents, 2)\n",
    "        states = jnp.array(agent_states)\n",
    "        # Compare each state to the center in a vectorized way.\n",
    "        is_center = jnp.all(states == self.center, axis=1)\n",
    "        rewards = jnp.where(is_center, self.center_reward, 0.0)\n",
    "        # Convert back to a Python list of floats.\n",
    "        return list(rewards.tolist())\n",
    "\n",
    "    def is_terminal(self, agent_states: List[Tuple[int, int]]) -> bool:\n",
    "        \"\"\"\n",
    "        Determine if the episode should terminate.\n",
    "        The episode terminates if any agent reaches the center.\n",
    "        \n",
    "        Args:\n",
    "            agent_states (List[Tuple[int, int]]): List of (x, y) positions for each agent.\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if any agent is at the center.\n",
    "        \"\"\"\n",
    "        states = jnp.array(agent_states)\n",
    "        is_center = jnp.all(states == self.center, axis=1)\n",
    "        return bool(jnp.any(is_center))\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, agents: List, environment: Environment, num_episodes: int = 1000, max_steps: int = 100):\n",
    "        \"\"\"\n",
    "        Initialize the trainer.\n",
    "        \n",
    "        Args:\n",
    "            agents (List): A list of agent instances.\n",
    "            environment (Environment): An instance of the environment.\n",
    "            num_episodes (int): Number of episodes to train.\n",
    "            max_steps (int): Maximum steps per episode.\n",
    "        \"\"\"\n",
    "        self.agents = agents\n",
    "        self.env = environment\n",
    "        self.num_episodes = num_episodes\n",
    "        self.max_steps = max_steps\n",
    "\n",
    "    def _episode_step(self, state: jnp.ndarray, unused):\n",
    "        \"\"\"\n",
    "        A single time step for all agents using JAX vectorized operations.\n",
    "        \n",
    "        Args:\n",
    "            state: jnp.ndarray of shape (num_agents, 2) representing current agent positions.\n",
    "            unused: placeholder for scan (not used).\n",
    "        \n",
    "        Returns:\n",
    "            next_state: Updated state after one step.\n",
    "            info: Tuple containing (prev_state, actions, rewards, done).\n",
    "        \"\"\"\n",
    "        num_agents = state.shape[0]\n",
    "        \n",
    "        # --- Action Selection ---\n",
    "        # For each agent, call its choose_action function.\n",
    "        # Here we loop over agents (since the number of agents is small),\n",
    "        # but you can later try to re-write choose_action in a fully vectorized way.\n",
    "        actions = []\n",
    "        for i in range(num_agents):\n",
    "            # Convert the i-th agent's state (a JAX array) to a tuple for compatibility.\n",
    "            s = tuple(state[i].tolist())\n",
    "            action = self.agents[i].choose_action(s)\n",
    "            actions.append(action)\n",
    "        actions = jnp.array(actions)  # shape (num_agents,)\n",
    "\n",
    "        # --- Environment Step ---\n",
    "        # Each agent takes a step with its chosen action.\n",
    "        next_states = []\n",
    "        for i in range(num_agents):\n",
    "            # We assume agent.step returns a new state as a tuple.\n",
    "            ns = self.agents[i].step(int(actions[i]))\n",
    "            next_states.append(jnp.array(ns))\n",
    "        next_state = jnp.stack(next_states)  # shape (num_agents, 2)\n",
    "\n",
    "        # --- Compute Rewards & Terminal Condition ---\n",
    "        # Convert next_states to a list of tuples for the environment.\n",
    "        state_list = [tuple(s.tolist()) for s in next_state]\n",
    "        rewards = jnp.array(self.env.compute_rewards(state_list))  # shape (num_agents,)\n",
    "        done = self.env.is_terminal(state_list)\n",
    "\n",
    "        return next_state, (state, actions, rewards, done)\n",
    "\n",
    "    def train_jax(self):\n",
    "        \"\"\"\n",
    "        Run a single episode using JAX's vectorized loop (lax.scan).\n",
    "        \n",
    "        Returns:\n",
    "            final_states: The states of all agents after the episode.\n",
    "            scan_info: A tuple containing per-step info (states, actions, rewards, done flags).\n",
    "        \"\"\"\n",
    "        # Gather initial states from all agents as a JAX array of shape (num_agents, 2).\n",
    "        init_states = jnp.stack([jnp.array(self.agents[i].get_state()) for i in range(len(self.agents))])\n",
    "        \n",
    "        # Run the episode loop for max_steps using lax.scan.\n",
    "        final_states, scan_info = jax.lax.scan(self._episode_step, init_states, None, length=self.max_steps)\n",
    "        return final_states, scan_info\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Run the training loop over multiple episodes.\n",
    "        For each episode, the environment is reset and the vectorized (JAX) inner loop is run.\n",
    "        \"\"\"\n",
    "        for episode in range(self.num_episodes):\n",
    "            # Reset the environment and all agents.\n",
    "            _ = self.env.reset()\n",
    "            for agent in self.agents:\n",
    "                agent.reset(randomize=True)\n",
    "            \n",
    "            # Run the episode in a vectorized manner.\n",
    "            final_states, scan_info = self.train_jax()\n",
    "            # (scan_info contains a tuple of (states, actions, rewards, done) for each step)\n",
    "            \n",
    "            # (Here you could aggregate rewards, update logging, etc.)\n",
    "            print(f\"Episode {episode+1} finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = SingleAgent(grid_size=5, alpha=0.1, gamma=0.9, epsilon=0.1, seed=0)\n",
    "env = GridCenterReward(grid_size=5, center_reward=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = Trainer(agents = [agent], environment = env, num_episodes = 1000, max_steps = 100)\n",
    "report = tr.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
