{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "\n",
    "# --- Batched Agent State ---\n",
    "\n",
    "@jax.tree_util.register_pytree_node_class\n",
    "@dataclass\n",
    "class BatchSingleAgentState:\n",
    "    x: jnp.ndarray       # shape: (n_agents,)\n",
    "    y: jnp.ndarray       # shape: (n_agents,)\n",
    "    q_table: jnp.ndarray # shape: (n_agents, grid_size, grid_size, num_actions)\n",
    "    key: jnp.ndarray     # shape: (n_agents, ...)\n",
    "\n",
    "    def tree_flatten(self):\n",
    "        # The children are the arrays; there is no auxiliary static data.\n",
    "        children = (self.x, self.y, self.q_table, self.key)\n",
    "        aux_data = None\n",
    "        return children, aux_data\n",
    "\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        return cls(*children)\n",
    "\n",
    "# --- Abstract Base Classes ---\n",
    "\n",
    "class Agent(ABC):\n",
    "    @abstractmethod\n",
    "    def init_state_batch(self, n_agents: int, randomize: bool = True) -> BatchSingleAgentState:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def choose_action_batch(self, state: BatchSingleAgentState) -> Tuple[jnp.ndarray, BatchSingleAgentState]:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def move_batch(self, state: BatchSingleAgentState, actions: jnp.ndarray) -> BatchSingleAgentState:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def update_batch(self, state: BatchSingleAgentState, actions: jnp.ndarray,\n",
    "                       rewards: jnp.ndarray, next_state: BatchSingleAgentState) -> BatchSingleAgentState:\n",
    "        pass\n",
    "\n",
    "\n",
    "class Environment(ABC):\n",
    "    @abstractmethod\n",
    "    def reset(self) -> Dict:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def compute_rewards_batch(self, state: BatchSingleAgentState) -> jnp.ndarray:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def is_terminal_batch(self, state: BatchSingleAgentState) -> bool:\n",
    "        pass\n",
    "\n",
    "\n",
    "# --- Concrete Implementations ---\n",
    "\n",
    "class SingleAgent(Agent):\n",
    "    def __init__(self, grid_size: int, alpha: float = 0.1, gamma: float = 0.9,\n",
    "                 epsilon: float = 0.1, seed: int = 0):\n",
    "        self.grid_size = grid_size\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.num_actions = 5  # 0:stay, 1:up, 2:down, 3:left, 4:right\n",
    "        self.seed = seed\n",
    "\n",
    "    def init_state_batch(self, n_agents: int, randomize: bool = True) -> BatchSingleAgentState:\n",
    "        base_key = jax.random.PRNGKey(self.seed)\n",
    "        # Split the base key into n_agents keys\n",
    "        agent_keys = jax.random.split(base_key, n_agents)\n",
    "        def init_fn(key):\n",
    "            # Split key into three parts: two for random position and one to carry forward.\n",
    "            key, subkey_x, subkey_y = jax.random.split(key, 3)\n",
    "            if randomize:\n",
    "                x = jax.random.randint(subkey_x, (), 0, self.grid_size, dtype=jnp.int32)\n",
    "                y = jax.random.randint(subkey_y, (), 0, self.grid_size, dtype=jnp.int32)\n",
    "            else:\n",
    "                x = jnp.array(self.grid_size // 2, dtype=jnp.int32)\n",
    "                y = jnp.array(self.grid_size // 2, dtype=jnp.int32)\n",
    "            return x, y, key\n",
    "        xs, ys, new_keys = jax.vmap(init_fn)(agent_keys)\n",
    "        q_table = jnp.zeros((n_agents, self.grid_size, self.grid_size, self.num_actions))\n",
    "        return BatchSingleAgentState(x=xs, y=ys, q_table=q_table, key=new_keys)\n",
    "\n",
    "    def choose_action_batch(self, state: BatchSingleAgentState) -> Tuple[jnp.ndarray, BatchSingleAgentState]:\n",
    "        def choose_fn(x, y, q_table, key):\n",
    "            key, subkey = jax.random.split(key)\n",
    "            rand_val = jax.random.uniform(subkey)\n",
    "            # Branch: either choose a random action or choose argmax of Q-values.\n",
    "            def random_branch(key):\n",
    "                key, subkey2 = jax.random.split(key)\n",
    "                action = jax.random.randint(subkey2, (), 0, self.num_actions)\n",
    "                return action, key\n",
    "            def exploit_branch(key):\n",
    "                action = jnp.argmax(q_table[x, y, :])\n",
    "                return action, key\n",
    "            action, new_key = jax.lax.cond(rand_val < self.epsilon,\n",
    "                                           random_branch,\n",
    "                                           exploit_branch,\n",
    "                                           operand=key)\n",
    "            return action, new_key\n",
    "        actions, new_keys = jax.vmap(choose_fn)(state.x, state.y, state.q_table, state.key)\n",
    "        new_state = BatchSingleAgentState(x=state.x, y=state.y, q_table=state.q_table, key=new_keys)\n",
    "        return actions, new_state\n",
    "\n",
    "    def move_batch(self, state: BatchSingleAgentState, actions: jnp.ndarray) -> BatchSingleAgentState:\n",
    "        def move_fn(x, y, action):\n",
    "            grid_size = self.grid_size\n",
    "            def stay():\n",
    "                return x, y\n",
    "            def up():\n",
    "                return x, jnp.maximum(y - 1, 0)\n",
    "            def down():\n",
    "                return x, jnp.minimum(y + 1, grid_size - 1)\n",
    "            def left():\n",
    "                return jnp.maximum(x - 1, 0), y\n",
    "            def right():\n",
    "                return jnp.minimum(x + 1, grid_size - 1), y\n",
    "            new_x, new_y = jax.lax.switch(action, [stay, up, down, left, right])\n",
    "            return new_x, new_y\n",
    "        new_x, new_y = jax.vmap(move_fn)(state.x, state.y, actions)\n",
    "        return BatchSingleAgentState(x=new_x, y=new_y, q_table=state.q_table, key=state.key)\n",
    "\n",
    "    def update_batch(self, state: BatchSingleAgentState, actions: jnp.ndarray,\n",
    "                     rewards: jnp.ndarray, next_state: BatchSingleAgentState) -> BatchSingleAgentState:\n",
    "        def update_fn(x, y, q_table, action, reward, next_x, next_y):\n",
    "            current_q = q_table[x, y, action]\n",
    "            max_next_q = jnp.max(q_table[next_x, next_y, :])\n",
    "            new_q = (1 - self.alpha) * current_q + self.alpha * (reward + self.gamma * max_next_q)\n",
    "            new_q_table = q_table.at[x, y, action].set(new_q)\n",
    "            return new_q_table\n",
    "        new_q_table = jax.vmap(update_fn)(\n",
    "            state.x, state.y, state.q_table, actions, rewards, next_state.x, next_state.y\n",
    "        )\n",
    "        return BatchSingleAgentState(x=state.x, y=state.y, q_table=new_q_table, key=state.key)\n",
    "\n",
    "\n",
    "class GridCenterReward(Environment):\n",
    "    def __init__(self, grid_size: int, center_reward: float = 1.0):\n",
    "        self.grid_size = grid_size\n",
    "        self.center = jnp.array([grid_size // 2, grid_size // 2], dtype=jnp.int32)\n",
    "        self.center_reward = center_reward\n",
    "\n",
    "    def reset(self) -> Dict:\n",
    "        return {\"grid_size\": self.grid_size, \"center\": self.center}\n",
    "\n",
    "    def compute_rewards_batch(self, state: BatchSingleAgentState) -> jnp.ndarray:\n",
    "        positions = jnp.stack([state.x, state.y], axis=1)  # shape: (n_agents, 2)\n",
    "        is_center = jnp.all(positions == self.center, axis=1)\n",
    "        rewards = jnp.where(is_center, self.center_reward, 0.0)\n",
    "        return rewards\n",
    "\n",
    "    def is_terminal_batch(self, state: BatchSingleAgentState) -> bool:\n",
    "        positions = jnp.stack([state.x, state.y], axis=1)\n",
    "        is_center = jnp.all(positions == self.center, axis=1)\n",
    "        return jnp.any(is_center)\n",
    "\n",
    "\n",
    "# --- Trainer using JAX's vectorized scan ---\n",
    "class Trainer:\n",
    "    def __init__(self, agent: SingleAgent, environment: GridCenterReward,\n",
    "                 n_agents: int, num_episodes: int = 1000, max_steps: int = 100):\n",
    "        self.agent = agent\n",
    "        self.env = environment\n",
    "        self.n_agents = n_agents\n",
    "        self.num_episodes = num_episodes\n",
    "        self.max_steps = max_steps\n",
    "\n",
    "    def _episode_step(self, state: BatchSingleAgentState, _):\n",
    "        # Choose actions in a batched, jitted way.\n",
    "        actions, state_after_choice = self.agent.choose_action_batch(state)\n",
    "        # Update positions based on actions.\n",
    "        new_state = self.agent.move_batch(state_after_choice, actions)\n",
    "        # Compute rewards for the new positions.\n",
    "        rewards = self.env.compute_rewards_batch(new_state)\n",
    "        # Update Q-table based on transition.\n",
    "        updated_state = self.agent.update_batch(state, actions, rewards, new_state)\n",
    "        # For logging, we return positions (from state.x, state.y), actions, and rewards.\n",
    "        return updated_state, (state.x, state.y, actions, rewards)\n",
    "\n",
    "    def train_episode(self, state: BatchSingleAgentState):\n",
    "        final_state, scan_info = jax.lax.scan(self._episode_step, state, None, length=self.max_steps)\n",
    "        return final_state, scan_info\n",
    "\n",
    "    def train(self):\n",
    "        for episode in range(self.num_episodes):\n",
    "            _ = self.env.reset()\n",
    "            # Initialize the batched state for all agents.\n",
    "            state = self.agent.init_state_batch(self.n_agents, randomize=True)\n",
    "            final_state, scan_info = self.train_episode(state)\n",
    "            # For example, we log the final positions.\n",
    "            final_positions = jnp.stack([final_state.x, final_state.y], axis=1)\n",
    "            print(f\"Episode {episode+1} finished\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 finished\n",
      "Episode 2 finished\n",
      "Episode 3 finished\n",
      "Episode 4 finished\n",
      "Episode 5 finished\n",
      "Episode 6 finished\n",
      "Episode 7 finished\n",
      "Episode 8 finished\n",
      "Episode 9 finished\n",
      "Episode 10 finished\n"
     ]
    }
   ],
   "source": [
    "grid_size = 5\n",
    "n_agents = 1  # Increase number of agents to better amortize JAX overhead.\n",
    "agent = SingleAgent(grid_size=grid_size, alpha=0.1, gamma=0.9, epsilon=0.1, seed=42)\n",
    "env = GridCenterReward(grid_size=grid_size, center_reward=1.0)\n",
    "trainer = Trainer(agent=agent, environment=env, n_agents=n_agents, num_episodes=10, max_steps=20)\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
